# Deep Reinforcement Learning Agent for Crafter

## ğŸ“Œ Objective
DÃ©velopper un agent d'apprentissage par renforcement profond (DRL) capable de jouer au jeu **Crafter** de maniÃ¨re plus performante qu'une politique alÃ©atoire. Ce projet explore des dÃ©fis comme lâ€™exploration, lâ€™assignation de crÃ©dit Ã  long terme, et la gÃ©nÃ©ralisation dans un environnement procÃ©dural complexe.

## ğŸ® Environnement : Crafter
Crafter est une version simplifiÃ©e en 2D de Minecraft, avec :
- GÃ©nÃ©ration procÃ©durale de carte Ã  chaque Ã©pisode
- Un arbre technologique complexe
- Un cadre idÃ©al pour tester les capacitÃ©s des agents Ã  apprendre des politiques de long terme

> ğŸ“¦ [Crafter GitHub](https://github.com/danijar/crafter)

---

## ğŸ§  ImplÃ©mentations

### ğŸ”¸ Random Agent
- Baseline naÃ¯ve : les actions sont choisies alÃ©atoirement.
- Sert de rÃ©fÃ©rence de performance minimale.

![Random Agent Performance](images/random_agent_plot.png)

---

### ğŸ”¹ Deep Q-Network (DQN)
- Approximation des Q-valeurs via un rÃ©seau de neurones.
- Apprentissage Ã  partir dâ€™expÃ©riences prÃ©cÃ©dentes (Replay Buffer).
- Politique Îµ-greedy pour lâ€™exploration.

![DQN Performance](images/dqn_plot.png)

---

### ğŸ”¹ Double DQN
- Corrige la surestimation des Q-valeurs dans DQN.
- Utilise deux rÃ©seaux : un pour la sÃ©lection dâ€™action, un pour lâ€™estimation de valeur.

![Double DQN Performance](images/double_dqn_plot.png)

---

### ğŸ”¹ Double Dueling DQN + Categorized Loss
- Architecture **Dueling** : sÃ©paration des flux de valeur et dâ€™avantage.
- **Double DQN** pour stabilitÃ©.
- **Categorized Loss** : prioritÃ© donnÃ©e aux erreurs dans les zones critiques â†’ meilleure efficacitÃ© dâ€™apprentissage.

![Double Dueling DQN with Categorized Loss Performance](images/double_dueling_categorized_plot.png)

---

## ğŸ“ˆ RÃ©sultats

| Agent                              | Reward moyen / Ã©pisode |
|-----------------------------------|-------------------------|
| Random Agent                      | ~X.XX                   |
| DQN                               | ~X.XX                   |
| Double DQN                        | ~X.XX                   |
| Double Dueling DQN + Cat. Loss    | **~X.XX**               |

> Les performances sont moyennÃ©es sur plusieurs runs avec des seeds diffÃ©rentes.

---

## ğŸ”§ Fichiers

- `train.py` : point dâ€™entrÃ©e pour lâ€™entraÃ®nement (`python train.py`)
- `agents/` : implÃ©mentations des diffÃ©rentes architectures (DQN, Double DQN, etc.)
- `analysis/plot_eval_performance.py` : script pour tracer les courbes de performance
- `logdir/` : dossiers de log pour chaque agent
- `images/` : graphiques des rÃ©sultats

---

## ğŸ“ Installation et ExÃ©cution

```bash
# Installation des dÃ©pendances
pip install -r requirements.txt

# EntraÃ®ner l'agent avec les meilleurs hyperparamÃ¨tres
python train.py

# Visualiser les performances
python analysis/plot_eval_performance.py --logdir logdir/my_agent

## ğŸ‘¥ Auteurs

- **Simon Illouz--Laurent**
- **Aubin-Bonnefoy**
---

## ğŸ’¡ IdÃ©es et extensions possibles

- EntraÃ®nement Ã  partir de **dÃ©monstrations humaines**
---

## ğŸ“š RÃ©fÃ©rences

- Danijar Hafner, *"Benchmarking the Spectrum of Agent Capabilities"*, 2021.
- H. van Hasselt et al., *"Deep Reinforcement Learning with Double Q-learning"*, 2016.
- Ziyun Wang et al., *"Dueling Network Architectures for Deep RL"*, 2016.
- Marc G. Bellemare et al., *"A Distributional Perspective on Reinforcement Learning"*, 2017.
